import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device:{device}')

transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5],[0.5]) 
])
train_dataset=torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=256, shuffle=True)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model=nn.Sequential(
            nn.Linear(100,7*7*256),
            nn.BatchNorm1d(7*7*256),
            nn.LeakyReLU(0.2),
            nn.Unflatten(1,(256,7,7)),
            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(128,64, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(64,1, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.Tanh()
        )
    def forward(self,x):
        return self.model(x)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model=nn.Sequential(
            nn.Conv2d(1,64,kernel_size=5, stride=2, padding=2),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Conv2d(64,128,kernel_size=5,stride=2,padding=2),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Flatten(),
            nn.Linear(128*7*7,1),
            nn.Sigmoid()
        )
    def forward(self,x):
        return self.model(x)

generator=Generator().to(device)
discriminator=Discriminator().to(device)

criterion=nn.BCELoss()
generator_optimizer=optim.Adam(generator.parameters(), lr=1e-4)
discriminator_optimizer=optim.Adam(discriminator.parameters(), lr=1e-4)

ef generate_noise(batch_size, noise_dim):
    return torch.randn(batch_size, noise_dim,device=device)

def train_step(real_images):
    batch_size=real_images.size(0)
    #train discriminator
    noise=generate_noise(batch_size,100)
    fake_images=generator(noise)
    real_labels=torch.ones(batch_size,1, device=device)
    fake_labels=torch.zeros(batch_size,1,device=device)
    real_images=real_images.to(device)
    
    real_output=discriminator(real_images)
    fake_output=discriminator(fake_images.detach())
    real_loss=criterion(real_output,real_labels)
    fake_loss=criterion(fake_output,fake_labels)
    discriminator_loss=real_loss+fake_loss
    discriminator_optimizer.zero_grad()
    discriminator_loss.backward()
    discriminator_optimizer.step()

    noise=generate_noise(batch_size,100)
    fake_images=generator(noise)
    fake_output=discriminator(fake_images)
    generator_loss=criterion(fake_output,real_labels)
    generator_optimizer.zero_grad()
    generator_loss.backward()
    generator_optimizer.step()
    return discriminator_loss.item(), generator_loss.item()

def generate_and_save_images(epoch,fixed_noise):
    fake_images=generator(fixed_noise).cpu().detach()
    fake_images=(fake_images+1)/2.0
    fig,axes=plt.subplots(4,4,figsize=(4,4))
    for i, ax in enumerate(axes.flatten()):
        ax.imshow(fake_images[i,0,:,:],cmap='gray')
        ax.axis('off')
    plt.savefig(f'images at epoch{epoch:04d}.png')
    plt.show()

EPOCHS=100
noise_dim=100
num_examples_to_generate=16
fixed_noise=generate_noise(num_examples_to_generate,noise_dim)

for epoch in range(EPOCHS):
    total_d_loss=0
    total_g_loss=0
    for real_images, _ in train_loader:
        d_loss,g_loss=train_step(real_images)
        total_d_loss+=d_loss
        total_g_loss +=g_loss
   
    print(f'Epoch {epoch+1}/{EPOCHS} | DLoss: {total_d_loss/len(train_loader):.4f} | {total_g_loss / len(train_loader):.4f}')
    if (epoch+1)%10==0:
        generate_and_save_images(epoch+1, fixed_noise)
